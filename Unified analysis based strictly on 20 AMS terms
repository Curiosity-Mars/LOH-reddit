# ===================================================================
# Final integrated version (public-release)
# Unified analysis based strictly on 20 AMS terms
# ===================================================================

# --- Import necessary libraries ---
import pandas as pd
import re
import networkx as nx
from itertools import combinations
from collections import Counter
from tqdm import tqdm
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive, files

# --- Step 0: Mount Google Drive and define file path ---
try:
    drive.mount('/content/drive')
    # !!! Please adjust this path to match your text file location !!!
    file_path = '/content/drive/MyDrive/AMS_corpus_public.txt'
except Exception as e:
    print(f"Failed to mount Google Drive: {e}")
    print("Check whether the file path is correct and access is authorized.")


# --- Critical revision point ---
# The analysis is strictly limited to the 20 AMS terms defined in the manuscript.
# This list forms the foundation for all subsequent computations.
ams_20_terms = [
    # Psychological (5 terms)
    "sleep", "anxiety", "mood", "depression", "irritability",
    # Sexual (5 terms)
    "libido", "desire", "sexual", "erection", "morning",
    # Somatic / Musculoskeletal (10 terms)
    "fatigue", "pain", "muscle", "strength", "back",
    "joint", "sweating", "hot", "performance", "peak"
]

# --- Step 1: Text preprocessing ---
print("Reading and processing text file...")
try:
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        text = f.read().lower()
    tokens = re.findall(r'\b[a-z]+\b', text)
    print("✅ Text processing complete.")
except FileNotFoundError:
    print(f"Error: File '{file_path}' not found. Please verify the path.")
    exit()


# ===================================================================
# Stage 1: Vocabulary Frequency Count (Table 2)
# ===================================================================
print("\n--- Running Stage 1: Vocabulary Frequency Count ---")
word_counts = Counter(tokens)
ams_freq = {term: word_counts.get(term, 0) for term in ams_20_terms}

# Convert results to DataFrame and export
df_freq = pd.DataFrame(list(ams_freq.items()), columns=['Term', 'Frequency'])
df_freq = df_freq.sort_values(by='Frequency', ascending=False).reset_index(drop=True)

print("Frequency count for 20 AMS terms:")
print(df_freq)

freq_output_path = "/content/AMS_frequency_20_terms_public.csv"
df_freq.to_csv(freq_output_path, index=False)
files.download(freq_output_path)
print(f"✅ Frequency results saved to {freq_output_path}")


# ===================================================================
# Stage 2: Centrality Analysis (Figure 1 data)
# ===================================================================
print("\n--- Running Stage 2: Centrality Analysis ---")
window_size = 5
co_occurrences = Counter()

print("Calculating co-occurrences...")
ams_20_terms_set = set(ams_20_terms)

for i in tqdm(range(len(tokens) - window_size + 1)):
    window = tokens[i:i+window_size]
    present_terms = {t for t in window if t in ams_20_terms_set}
    for a, b in combinations(sorted(present_terms), 2):
        co_occurrences[(a, b)] += 1

# Build the network graph
G = nx.Graph()
# Add only edges with ≥5 co-occurrences (to reduce noise)
for (a, b), w in co_occurrences.items():
    if w >= 5:
        G.add_edge(a, b, weight=w)

print(f"✅ Graph constructed with {len(G.nodes())} nodes and {len(G.edges())} edges.")

# Compute centrality measures
print("Calculating centrality metrics...")
degree_c = nx.degree_centrality(G)
betweenness_c = nx.betweenness_centrality(G, weight='weight')
eigen_c = nx.eigenvector_centrality(G, weight='weight', max_iter=1000)

# Summarize results
centrality_results = []
for term in G.nodes():
    centrality_results.append({
        'Term': term,
        'Degree': degree_c.get(term, 0),
        'Betweenness': betweenness_c.get(term, 0),
        'Eigenvector': eigen_c.get(term, 0)
    })

df_centrality = pd.DataFrame(centrality_results).sort_values(by='Degree', ascending=False).reset_index(drop=True)

print("Centrality analysis results for AMS terms:")
print(df_centrality.head(10))

centrality_output_path = '/content/AMS_centrality_20_terms_public.csv'
df_centrality.to_csv(centrality_output_path, index=False)
files.download(centrality_output_path)
print(f"✅ Centrality results saved to {centrality_output_path}")


# ===================================================================
# Stage 3: Visualization (Figure 1 Panels a–c)
# ===================================================================
print("\n--- Running Stage 3: Generating Figure 1 ---")

top_10_terms = df_centrality.head(10)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

term_order = top_10_terms.sort_values(by='Degree', ascending=False)['Term'].tolist()
metrics = ['Degree', 'Betweenness', 'Eigenvector']
titles = ['(a) Degree centrality', '(b) Betweenness centrality', '(c) Eigenvector centrality']

for ax, metric, title in zip(axes, metrics, titles):
    plot_data = df_centrality.sort_values(by=metric, ascending=False).head(10)
    sns.barplot(y='Term', x=metric, data=plot_data, ax=ax, color='skyblue', orient='h')
    ax.set_title(title, fontsize=12, weight='bold')
    ax.set_xlabel('')
    ax.set_ylabel('')

fig.tight_layout()
figure_output_path = '/content/Figure1_AMS_Centrality_Public.png'
plt.savefig(figure_output_path, dpi=300, bbox_inches='tight')
plt.show()

files.download(figure_output_path)
print(f"✅ Figure 1 saved to {figure_output_path}")
