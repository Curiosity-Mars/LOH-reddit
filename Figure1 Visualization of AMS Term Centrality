# ================================================================
# Stage 2: Clinical AMS Vocabulary Detection in Reddit Discourse
# ================================================================

# 1️⃣ Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2️⃣ Specify file path
# Example: /content/drive/MyDrive/AMS_Reddit_Corpus_Public.txt
file_path = '/content/drive/MyDrive/AMS_Reddit_Corpus_Public.txt'

# 3️⃣ Define 25 AMS-related terms (from Stage 1)
ams_terms = [
    "sleep", "anxiety", "mood", "depression", "irritability",
    "libido", "desire", "sexual", "erection", "morning",
    "fatigue", "pain", "muscle", "strength", "back",
    "joint", "sweating", "hot", "performance", "peak",
    "weakness", "tired", "exhaustion", "energy", "motivation"
]

# 4️⃣ Load text file
with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
    text = f.read().lower()

# 5️⃣ Count word frequencies
from collections import Counter
import re

tokens = re.findall(r'\b[a-z]+\b', text)
word_counts = Counter(tokens)
ams_freq = {term: word_counts[term] for term in ams_terms}

# 6️⃣ Convert to DataFrame
import pandas as pd
df = pd.DataFrame(list(ams_freq.items()), columns=['Term', 'Frequency'])
df = df.sort_values(by='Frequency', ascending=False)

# 7️⃣ Display detected terms and total counts
detected_terms = (df['Frequency'] > 0).sum()
total_freq = df['Frequency'].sum()
print(f"Detected AMS terms: {detected_terms} / {len(ams_terms)}")
print(f"Total occurrences: {total_freq}")

# 8️⃣ Save CSV and download automatically
output_path = "/content/AMS_vocab_detection_results.csv"
df.to_csv(output_path, index=False)

from google.colab import files
files.download(output_path)


# ================================================================
# Stage 2: Centrality Analysis of AMS Terms in Reddit Discourse
# ================================================================

import pandas as pd
import re
import networkx as nx
from itertools import combinations
from collections import Counter
from tqdm import tqdm

# 1️⃣ Read text again
file_path = '/content/drive/MyDrive/AMS_Reddit_Corpus_Public.txt'
with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
    text = f.read().lower()

# 2️⃣ Use the same 25 AMS terms as above
ams_terms = [
    "sleep", "anxiety", "mood", "depression", "irritability",
    "libido", "desire", "sexual", "erection", "morning",
    "fatigue", "pain", "muscle", "strength", "back",
    "joint", "sweating", "hot", "performance", "peak",
    "weakness", "tired", "exhaustion", "energy", "motivation"
]

# 3️⃣ Tokenize the text
tokens = re.findall(r'\b[a-z]+\b', text)

# 4️⃣ Extract co-occurring AMS pairs within 5-word window
window_size = 5
co_occurrences = Counter()
for i in tqdm(range(len(tokens) - window_size)):
    window = tokens[i:i+window_size]
    present_terms = [t for t in window if t in ams_terms]
    for a, b in combinations(sorted(set(present_terms)), 2):
        co_occurrences[(a, b)] += 1

# 5️⃣ Build co-occurrence network
G = nx.Graph()
for (a, b), w in co_occurrences.items():
    if w >= 5:  # noise threshold
        G.add_edge(a, b, weight=w)

# 6️⃣ Compute centrality metrics
degree_c = nx.degree_centrality(G)
betweenness_c = nx.betweenness_centrality(G, weight='weight')
eigen_c = nx.eigenvector_centrality_numpy(G, weight='weight')

# 7️⃣ Convert to DataFrame
df_centrality = pd.DataFrame({
    'Term': list(G.nodes()),
    'Degree': [degree_c[t] for t in G.nodes()],
    'Betweenness': [betweenness_c[t] for t in G.nodes()],
    'Eigenvector': [eigen_c[t] for t in G.nodes()]
}).sort_values(by='Degree', ascending=False)

df_centrality.reset_index(drop=True, inplace=True)
df_centrality.head(10)

# 8️⃣ Save CSV and download automatically
output_path = '/content/AMS_centrality_results.csv'
df_centrality.to_csv(output_path, index=False)

from google.colab import files
files.download(output_path)


# ================================================================
# Figure 1: Visualization of AMS Term Centrality
# ================================================================
import matplotlib.pyplot as plt
import seaborn as sns

# Use top 10 AMS terms from centrality table
top_terms = df_centrality.head(10)

fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)
metrics = ['Degree', 'Betweenness', 'Eigenvector']
titles  = ['(a) Degree centrality', '(b) Betweenness centrality', '(c) Eigenvector centrality']

for ax, metric, title in zip(axes, metrics, titles):
    sns.barplot(y='Term', x=metric, data=top_terms, ax=ax, color='skyblue')
    ax.set_title(title, fontsize=12, weight='bold')
    ax.set_xlabel('')
    ax.set_ylabel('')
    ax.invert_yaxis()

plt.tight_layout()
plt.savefig('/content/Figure1_AMS_Centrality.png', dpi=300, bbox_inches='tight')
plt.show()

from google.colab import files
files.download('/content/Figure1_AMS_Centrality.png')
